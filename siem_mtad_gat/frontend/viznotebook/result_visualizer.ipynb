{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADBox Result Visualizer \n",
    "This Jupyter notebook provides a detailed visualization of the results of the ADBox, a custom anomaly-based intrusion detection component used in the IDPS-ESCAPE system. The ADBox integrates with Wazuh to detect anomalies in time-series data using the MTAD-GAT algorithm. \n",
    "This notebook is designed to present the results of training the anomaly detection model and its subsequent results during prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites and Guidelines \n",
    "This notebook is designed for visualizing the results of anomaly detection using ADBox, not for executing the ADBox itself. To successfully visualize the results:\n",
    "\n",
    "1. Ensure ADBox Training and Prediction Outputs: Complete the training and prediction processes with ADBox separately.\n",
    "\n",
    "2. Provide File Paths: Supply the notebook with the file paths where the ADBox output files are stored. These file paths are necessary for the notebook to access and display the results accurately.\n",
    "\n",
    "Without the correct file paths, the notebook will be unable to print and plot the desired results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADBox \n",
    "ADBox is the anomaly detection (AD) subsystem of IDPS-ESCAPE. It handles the ingestion, preprocessing, and evaluation of data collected by the SIEM. ADBox takes data from the events indexer, applies an anomaly detection model, and returns the results. Using MTAD-GAT (Multivariate Time-series Anomaly Detection via Graph Attention Network), a self-supervised learning framework, ADBox detects anomalies in multivariate time-series data through machine learning. \n",
    "\n",
    "It runs through the following stages which are the two fundamental stages of machine learning: \n",
    "\n",
    "##### Training Stage\n",
    "1. **Data Ingestion:**\n",
    "During training, ADBox gathers time-series data from Wazuh. This data is used to teach the model how to identify normal patterns and distinguish them from anomalous behavior.\n",
    "2. **Preprocessing:**\n",
    "The collected data is preprocessed to ensure it is in a format suitable for training. This might include normalization, handling missing values, or reshaping the data.\n",
    "3. **Model Training:**\n",
    "The MTAD-GAT algorithm is applied to the preprocessed time-series data. \n",
    "4. **Visualization:**\n",
    "The notebook includes visualizations such as loss curves to illustrate the training performance and validate the model's effectiveness.\n",
    "\n",
    "##### Prediction/Detection Stage\n",
    "1. **Data Ingestion:**\n",
    "For prediction, the ADBox ingests new time-series data from Wazuh. \n",
    "2. **Preprocessing:**\n",
    "The prediction input data is also preprocessed. \n",
    "3. **Anomaly Detection:**\n",
    "The trained MTAD-GAT model is applied to the data to detect anomalies. It uses the learned patterns from the training phase to identify deviations from expected behavior. The model provides predictions indicating whether each time window in the data is normal or anomalous.\n",
    "4. **Visualization:**\n",
    "Visualizations in this notebook include time-series plots with detected anomalies highlighted, anomaly scores over time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import re\n",
    "from plotly.subplots import make_subplots\n",
    "import json \n",
    "import os \n",
    "import yaml\n",
    "from IPython.display import Image, display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables and paths to be modified \n",
    "The following variables should should be updated for the use of the notebook. \n",
    "1. The base path should specify the path where the assets directory is located. \n",
    "2. Use case is the number of the use case which was used for training or prediction. \n",
    "3. Detector id is the id of the detector which was trained or was used for prediction. The notebook will try to find the output files in the folder named after this detector id so it should be correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths to be modified \n",
    "base_path = \"/home/alab/siem-mtad-gat/siem_mtad_gat\"\n",
    "use_case = \"9\"\n",
    "detector_id = \"2d36a80a-c47a-4eb4-bb3e-5b2bfb90dc95\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variabled and paths not to be modified \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths not to be modified \n",
    "yaml_file_path = os.path.join(base_path, f'assets/drivers/uc_{use_case}.yaml') \n",
    "detector_root_directory = os.path.join(base_path, f'assets/detector_models/{detector_id}') \n",
    "detector_input_parameters_path = os.path.join(detector_root_directory, 'input/detector_input_parameters.json') \n",
    "training_config_path = os.path.join(detector_root_directory, 'input/training_config.json')\n",
    "train_losses_image_path = os.path.join(detector_root_directory, 'training/train_losses.png') \n",
    "validation_losses_image_path = os.path.join(detector_root_directory, 'training/validation_losses.png') \n",
    "train_output_path = os.path.join(detector_root_directory, 'training/train_output.pkl') \n",
    "prediction_directory = os.path.join(detector_root_directory, 'prediction')\n",
    "files = os.listdir(prediction_directory) \n",
    "predicted_anomalies_data_pattern = re.compile(rf'uc-{use_case}_predicted_anomalies_data-(\\d+)_.*\\.json')\n",
    "predicted_data_pattern = re.compile(rf'uc-{use_case}_predicted_data-(\\d+)_.*\\.json')\n",
    "max_number = -1\n",
    "predicted_data_pattern_max_file = None\n",
    "for file in files:\n",
    "    match = predicted_data_pattern.match(file)\n",
    "    if match:\n",
    "        number = int(match.group(1))\n",
    "        if number > max_number:\n",
    "            max_number = number\n",
    "            predicted_data_pattern_max_file = file\n",
    "    \n",
    "if predicted_data_pattern_max_file is not None: \n",
    "    prediction_output_path = os.path.join(prediction_directory, predicted_data_pattern_max_file) \n",
    "    print(f\"The prediction file used for visualization will be {predicted_data_pattern_max_file}. \")\n",
    "else: \n",
    "    print(\"No prediction file found. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the ADBox with a use case \n",
    "Use cases in the context of the ADBOx refer to the consolidation of training or prediction inputs in a file that are given to the ADBox. This provides a simple way of interacting with the ADBox since all the required values are provided using a single file. \n",
    "These files are user defined and should contain the right keys for their values to be processed by the ADBox instead of using the default values. \n",
    "\n",
    "If the training or prediction are performed using a use_case file, then the contents of the use case file contain the inputs that are provided to the training and prediction. \n",
    "A training and detection use case can be run by providing the uc flag along win a number to the script that runs the ADBox docker container. \n",
    "The yaml file should be present in the `/siem_mtad_gat/assets/drivers/` folder. \n",
    "```sh\n",
    "./adbox.sh -uc {use_case} \n",
    "``` \n",
    "With this input, the ADBox will take the inputs specified in the `uc_{use_case}.yaml` file. \n",
    "Once the script is run, it runs a training and prediction cycle if both keys are present in the yaml file. \n",
    "The contents of the yaml file can be read to see the inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the YAML file \n",
    "with open(yaml_file_path, 'r') as file:\n",
    "    yaml_content = yaml.safe_load(file)\n",
    "    \n",
    "yaml_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents of the yaml file \n",
    "The contents of the yaml file include the input parameters for training and prediction/anomaly detection. \n",
    "#### Training input parameters \n",
    "The training input parameters include: \n",
    "1. **index_date**: It represents the data source index where the training data should be fetched from. If it is specified as   `default`, it will use the default index_date, which would be the index date for the current month. \n",
    "2. **categorical_features**: Specifies that the given input features include categorical features or not. \n",
    "3. **columns**: This specifies a list of columns used as features to train the detector. \n",
    "4. **aggregation**: Specifies if the column values should be aggregated or not. \n",
    "5. **aggregation_config**: If the aggregation is set to  `True`, then an aggregation config is required to specify the configurations that the ADBox uses to perform the aggregation. This further contains. \n",
    "    1. **fill_na_method**: It is fill method to handle null values. \n",
    "    2. **padding_value**: Only required when the fill_na_method is `Fixed`. \n",
    "    3. **granularity**: The granularity to aggregate the input data. \n",
    "    4. **features**: It is a key value pair of the features and the method that they should use for aggregation. \n",
    "6. **train_config**: The train_config specifies the training configurations, It contains the  `window_size` and  `epochs`. \n",
    "7. **display_name**: Specifies a name for the detector. \n",
    "\n",
    "#### Prediction/detection input parameters \n",
    " The prediction input parameters include: \n",
    "1. **run_mode**: This value specifies the detection run mode. \n",
    "2. **index_date**: This is the date string that the detector will use to fetch data from the consequent Wazuh index. If it is specified as   `default`, it will use the default index_date, which would be the index date for the current day. \n",
    "3. **detector_id**: This is the detector ID for the detector selected for detection. If it is specified as  `default`, it will detect using the most recently trained detector.  \n",
    "4. **start_time**: This is the start time for detection. If it is `default`, it will be set to the starting timestamp of the current date.\n",
    "5. **end_time**: This is the end time for detection. If it is `default`, it will be set to the current timestamp of the current date. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training \n",
    "In the training process, the ADBox trains a model using the provided specifications. Each trained detector is identified by a unique id and all of its generated artifacts are stored in a folder named as the detector id. \n",
    "Reading the contents of the folder shows the following subfolders and files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(detector_root_directory):\n",
    "    # Print the directory name relative to root_dir\n",
    "    print(f\"Directory: {os.path.relpath(root, os.path.dirname(detector_root_directory))}\")\n",
    "    for file in files:\n",
    "        # Print the file name relative to root_dir\n",
    "        print(f\"  File: {os.path.relpath(os.path.join(root, file), detector_root_directory)}\")\n",
    "    for dir in dirs:\n",
    "        # Print the subdirectory name relative to root_dir\n",
    "        print(f\"  Subdirectory: {os.path.relpath(os.path.join(root, dir), detector_root_directory)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The contents of the folder generated for the trained detector after running the ADBox shows three subfolders input, training and prediction. \n",
    "#### 1. input\n",
    "The input folder contains the the following two file. \n",
    "##### a. detector_input_parameters.json: \n",
    "This file is generated as a result of the training input parameters provided in the yaml file.  While reading the file it could be seen that it contains the same fields as defined above for the yaml file and some other fields that were added after the training of the detector. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and print the contents of the JSON file\n",
    "with open(detector_input_parameters_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "    print(json.dumps(data, indent=4))  # Pretty-print the JSON data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents of the detector_input_parameters.json file \n",
    "The individual contents of this file have already been defined in the yaml file section. The additional fields such as, `created_time` represents the time at which the detector was created. And `model_info` provides some details about the training of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b. training_config.json: \n",
    "This file contains more details about the machine learning level training parameters. \n",
    "This file is critical for configuring how the MTAD-GAT machine learning model is trained. This specific configuration file includes parameters that define the model architecture, training process, and other hyperparameters. \n",
    "\n",
    "This file should be changed by the user if the user has sufficient knowledge about machine learning algorithms, parameters, their working and functionalities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and print the contents of the JSON file\n",
    "with open(training_config_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "    print(json.dumps(data, indent=4))  # Pretty-print the JSON data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents of the training_config.json file \n",
    "\n",
    "1. **window_size**: This parameter likely determines the size of the time window used for time-series data input.\n",
    "2. **spec_res**: Indicates whether to use spectral residuals, often used for anomaly detection in time-series data.\n",
    "3. **kernel_size**: The size of the kernel (filter) in convolutional layers, which impacts feature extraction.\n",
    "4. **use_gatv2**: Specifies whether to use a version 2 Graph Attention Network (GATv2), a type of neural network architecture for graph-based data.\n",
    "5. **feat_gat_embed_dim**: The embedding dimension for the feature-based Graph Attention Network, if applicable.\n",
    "6. **time_gat_embed_dim**: The embedding dimension for the time-based Graph Attention Network, if applicable.\n",
    "7. **gru_n_layers**: The number of layers in the Gated Recurrent Unit (GRU) network, a type of recurrent neural network.\n",
    "8. **gru_hid_dim**: The hidden dimension size of the GRU layers.\n",
    "9. **fc_n_layers**: The number of fully connected (dense) layers in the neural network.\n",
    "10. **fc_hid_dim**: The hidden dimension size of the fully connected layers.\n",
    "11. **recon_n_layers**: The number of layers in the reconstruction part of the network, possibly for autoencoders.\n",
    "12. **recon_hid_dim**: The hidden dimension size of the reconstruction layers.\n",
    "13. **alpha**: A coefficient, possibly for the learning rate or loss function scaling.\n",
    "14. **epochs**: The number of epochs, or full passes through the training dataset.\n",
    "15. **val_split**: The fraction of data to be used for validation.\n",
    "16. **bs**: Batch size, or the number of samples per gradient update.\n",
    "17. **init_lr**: Initial learning rate for the optimizer.\n",
    "18. **shuffle_dataset**: Whether to shuffle the dataset before each epoch.\n",
    "19. **dropout**: The dropout rate, a regularization technique to prevent overfitting by randomly setting a fraction of input units to zero during training.\n",
    "20. **use_cuda**: Indicates whether to use GPU acceleration (CUDA).\n",
    "21. **print_every**: Frequency of printing training progress.\n",
    "22. **log_tensorboard**: Whether to log metrics to TensorBoard, a visualization tool.\n",
    "23. **scale_scores**: Indicates if the scores should be scaled.\n",
    "24. **use_mov_av**: Whether to use moving average smoothing.\n",
    "25. **gamma**: A parameter for adjusting learning dynamics, possibly the learning rate decay.\n",
    "26. **level**: A level parameter, the context of which would depend on the specific algorithm.\n",
    "27. **q**: A parameter that could relate to quantization or another algorithm-specific function.\n",
    "28. **dynamic_pot**: Whether to use dynamic potential, possibly related to the dynamic adjustment of model parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. training\n",
    "The training folder contains the the following two file. \n",
    "##### a. train_output.pkl\n",
    "This file contains the saved forecasts, reconstructions, actual, thresholds, etc. on the training dataset in pickle format. \n",
    "##### b. test_output.pkl:  \n",
    "This file contains the saved forecasts, reconstructions, actual, thresholds, etc. on the testing dataset in pickle format. \n",
    "##### c. model.pt\n",
    "This file contains the model parameters of trained model in a .pt file which is a PyTorch file used to save and load model parameters, entire models, or tensor data. \n",
    "##### d. losses_train_data.json \n",
    "This file contains contains the training losses for each epoch. \n",
    "##### e. train_losses.png  \n",
    "A plot of train loss during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename=train_losses_image_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### f. validation_losses.png \n",
    "A plot of validation loss during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename=validation_losses_image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle(train_output_path)  \n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add red shadows for anomalies\n",
    "shaded=False\n",
    "\n",
    "# Extracting timestamps and scores\n",
    "timestamps = train_df.index\n",
    "scores = train_df.get('A_Score_Global') \n",
    "# Convert is_anomaly to 0 or 1\n",
    "is_anomaly = train_df.get(\"A_Pred_Global\")\n",
    "\n",
    "#threshold\n",
    "threshold = train_df.get(\"Thresh_Global\")\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({'Timestamp': timestamps, 'Score': scores, 'Is_Anomaly': is_anomaly, 'Threshold': threshold})\n",
    "\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add the score line with markers\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df['Timestamp'],\n",
    "    y=df['Score'],\n",
    "    mode='lines+markers',\n",
    "    text=df['Score'].apply(lambda x: f'Anomaly Score: {x:.2f}'),  # Hover text\n",
    "    hoverinfo='text',  # Show only text on hover\n",
    "    line=dict(color='#d53e1f', width=1),  # Line color\n",
    "    marker=dict(\n",
    "        size=4,  # Marker size\n",
    "        color='rgba(0,0,0,0)',  # Marker fill color (transparent)\n",
    "        line=dict(\n",
    "            width=1,  # Border width\n",
    "            color='#d53e1f'  # Border color (red)\n",
    "        ),\n",
    "    ), \n",
    "    name='Anomaly Score'\n",
    "))\n",
    "\n",
    "\n",
    "# Add the threshold line\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df['Timestamp'],\n",
    "    y=df['Threshold'],\n",
    "    mode='lines+markers',\n",
    "    line=dict(color='black', width=1),  # Blue line\n",
    "    name='Threshold', \n",
    "    marker=dict(\n",
    "        size=4,  # Marker size\n",
    "        color='rgba(0,0,0,0)',  # Marker fill color (transparent)\n",
    "        line=dict(\n",
    "            width=1,  # Border width\n",
    "            color='black'  # Border color (red)\n",
    "        ),\n",
    "    ), \n",
    "    #showlegend=False  # Do not show legend for the anomaly line\n",
    "))\n",
    "\n",
    "\n",
    "\n",
    "if shaded:\n",
    "    # Add shaded areas for anomalies\n",
    "    for i in range(len(df)):\n",
    "        if df['Is_Anomaly'][i] == 1:\n",
    "            start = df['Timestamp'][i]\n",
    "            end = df['Timestamp'][i + 1] if i + 1 < len(df) else df['Timestamp'][i]\n",
    "            fig.add_shape(\n",
    "                type=\"rect\",\n",
    "                x0=start,\n",
    "                x1=end,\n",
    "                y0=0,\n",
    "                y1=1,\n",
    "                xref='x',\n",
    "                yref='paper',\n",
    "                fillcolor='red',\n",
    "                opacity=0.2,\n",
    "                line_width=0,\n",
    "            )\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Anomaly Scores and Prediction Over Time',\n",
    "    xaxis_title='Timestamp',\n",
    "    yaxis_title='Anomaly Score',\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction  \n",
    "After the training is finished, the use case runs a detection \n",
    "#### 3. prediction \n",
    "\n",
    "The prediction folder would contain  output JSON file for each time the prediction is run specifying the number of the use case and timestamp that it was run at. Each run would generate two files as output, one file having the predicted data for all the data points and the other having the predicted data only for the points which were predicted as anomalies. \n",
    "\n",
    "##### a. uc-{use_case}_predicted_data-{n}_{timestamp}.json \n",
    "This file contains predicted data for all the points that were used for prediction, and flags them as anomalies or not. \n",
    "\n",
    "##### b. uc-{use_case}_predicted_anomalies_data-{n}_{timestamp}.json \n",
    "This files contains the anomalies detected during the prediction run. \n",
    "\n",
    "The contents of both of these files would be in a similar format only differntiating in the anomaly flag being true or false. The `uc-{use_case}_predicted_anomalies_data-{n}_{timestamp}.json` file will contains have all the points all tha points which have the `is_anomaly` flag `true`, whereas `uc-{use_case}_predicted_data-{n}_{timestamp}.json` would have points with `is_anomaly` flag `true` and `false` both. \n",
    "\n",
    "For the plotting, `uc-{use_case}_predicted_data-{n}_{timestamp}.json` will be used and for the sake of explanation as well, we only explain the fields in the `uc-{use_case}_predicted_data-{n}_{timestamp}.json` file, as the other one would have the same format.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and print the contents of the JSON file\n",
    "with open(prediction_output_path, 'r') as file:\n",
    "    prediction_output_data = json.load(file)\n",
    "    print(json.dumps(prediction_output_data, indent=4))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaining the result fields \n",
    "Before proceeding to plot and visualize these values, a description of each field in the result object and its significance is provided. We only discuss the result object, because otherwise the fields above the result object are the same which were given as input for the prediction. \n",
    " \n",
    "- **timestamp:** Represents the date and time when the data was recorded, it is the start time of the window in which the anomaly data point was recorded. This timestamp indicates the specific moment the observation was made. \n",
    "The output will contain each point based upon the input granularity for example, if the granularity was 1 minute, then the output will contain a datapoint after every minute since the data was aggregated using the input granularity. \n",
    "\n",
    "- **is_anomaly:** A boolean value indicating whether the data point is classified as an anomaly. false signifies that the data point is not considered an anomaly. \n",
    "- **score:** The global anomaly score for the data point, which quantifies how anomalous the data is.  \n",
    "- **prediction_values:** A dictionary containing various metrics related to the forecast, reconstruction, and anomaly scores: \n",
    "    For each feature, the dictionary contains the following values: \n",
    "    - **Forecast:** The forecast refers to the predicted values for the next timestamp in a time-series for the feature. \n",
    "    - **Recon:** The reconstructed value of the feature. \n",
    "    - **True:** The actual value of feature from the data.\n",
    "    - **A_Score:** The anomaly score for the feature. \n",
    "    - **Thresh:** The threshold value used to determine if the feature is considered an anomaly. \n",
    "    - **A_Pred:** Anomaly prediction for the feature, where 0 indicates no anomaly and 1 indicates an anomaly.  It is 1 if its corresponding anomaly score is larger than or equal to the threshold. \n",
    "\n",
    "    And the following global values: \n",
    "    - **A_Score_Global:** The global anomaly score that aggregates the anomaly scores across all features. \n",
    "    - **Thresh_Global:** The global threshold used to determine if the aggregated anomaly score is considered an anomaly. \n",
    "    - **A_Pred_Global:** Anomaly prediction based on the global anomaly score. Here, 0 indicates no anomaly, and 1 indicates an anomaly. It is 1 if its corresponding anomaly score is larger than or equal to the threshold. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the detected anomalies \n",
    "An anomaly refers to a data point or pattern in a dataset that significantly deviates from the expected or normal behavior. \n",
    "Here, the anomalies would be data points which were observed to have anomalous values based upon the features which were used to train the detector. \n",
    "\n",
    "The following cell fetches the results section from the output file which would be used to plot the following plots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_output = []\n",
    "for entry in prediction_output_data: \n",
    "    predict_output.extend(entry.get('results', [])) \n",
    "    \n",
    "    \n",
    "# Specify start and end time for filtering (example values)\n",
    "start_time = datetime(2024, 8, 30, 5, 0, tzinfo=timezone.utc)  # YYYY, MM, DD, HH, MM\n",
    "end_time = datetime(2024, 8, 30, 23, 0, tzinfo=timezone.utc)    # YYYY, MM, DD, HH, MM\n",
    "\n",
    "# Filter the data before converting it into a DataFrame\n",
    "predict_output = [\n",
    "    result for result in predict_output \n",
    "    if start_time <= datetime.fromisoformat(result['timestamp'].replace('Z', '+00:00')) <= end_time\n",
    "] \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly scores and Prediction over time \n",
    "The following chart shows the global anomaly scores for each data point which are plotted against the timestamp they were predicted at. The red area signifies the points which were predicted as anomalies. And the `Is Anomaly` value shows if the individual point was flagged as an anomaly or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each entry in the list\n",
    "results = predict_output\n",
    "    \n",
    "\n",
    "# Extracting timestamps and scores\n",
    "timestamps = [datetime.fromisoformat(result['timestamp'].replace('Z', '+00:00')) for result in results]\n",
    "scores = [result['score'] for result in results] \n",
    "# Convert is_anomaly to 0 or 1\n",
    "is_anomaly = [1 if result['is_anomaly'] else 0 for result in results]\n",
    "\n",
    "#threshold\n",
    "threshold = [result.get(\"prediction_values\").get(\"Thresh_Global\") for result in results]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({'Timestamp': timestamps, 'Score': scores, 'Is_Anomaly': is_anomaly, 'Threshold': threshold})\n",
    "\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add the score line with markers\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df['Timestamp'],\n",
    "    y=df['Score'],\n",
    "    mode='lines+markers',\n",
    "    text=df['Score'].apply(lambda x: f'Anomaly Score: {x:.2f}'),  # Hover text\n",
    "    hoverinfo='text',  # Show only text on hover\n",
    "    line=dict(color='#d53e1f', width=1),  # Line color\n",
    "    marker=dict(\n",
    "        size=4,  # Marker size\n",
    "        color='rgba(0,0,0,0)',  # Marker fill color (transparent)\n",
    "        line=dict(\n",
    "            width=1,  # Border width\n",
    "            color='#d53e1f'  # Border color (red)\n",
    "        ),\n",
    "    ), \n",
    "    name='Anomaly Score'\n",
    "))\n",
    "\"\"\"\n",
    "# Add the is_anomaly line\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df['Timestamp'],\n",
    "    y=df['Is_Anomaly'],\n",
    "    mode='lines+markers',\n",
    "    line=dict(color='green', width=1),  # Black line\n",
    "    name='Is Anomaly', \n",
    "    marker=dict(\n",
    "        size=4,  # Marker size\n",
    "        color='rgba(0,0,0,0)',  # Marker fill color (transparent)\n",
    "        line=dict(\n",
    "            width=1,  # Border width\n",
    "            color='green'  # Border color (red)\n",
    "        ),\n",
    "    ), \n",
    "    #showlegend=False  # Do not show legend for the anomaly line\n",
    "))\n",
    "\"\"\"\n",
    "# Add the threshold line\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df['Timestamp'],\n",
    "    y=df['Threshold'],\n",
    "    mode='lines+markers',\n",
    "    line=dict(color='black', width=1),  # Blue line\n",
    "    name='Threshold', \n",
    "    marker=dict(\n",
    "        size=4,  # Marker size\n",
    "        color='rgba(0,0,0,0)',  # Marker fill color (transparent)\n",
    "        line=dict(\n",
    "            width=1,  # Border width\n",
    "            color='black'  # Border color (red)\n",
    "        ),\n",
    "    ), \n",
    "    #showlegend=False  # Do not show legend for the anomaly line\n",
    "))\n",
    "\n",
    "\n",
    "# Add shaded areas for anomalies\n",
    "for i in range(len(df)):\n",
    "    if df['Is_Anomaly'][i] == 1:\n",
    "        start = df['Timestamp'][i]\n",
    "        end = df['Timestamp'][i + 1] if i + 1 < len(df) else df['Timestamp'][i]\n",
    "        fig.add_shape(\n",
    "            type=\"rect\",\n",
    "            x0=start,\n",
    "            x1=end,\n",
    "            y0=0,\n",
    "            y1=1,\n",
    "            xref='x',\n",
    "            yref='paper',\n",
    "            fillcolor='red',\n",
    "            opacity=0.2,\n",
    "            line_width=0,\n",
    "        )\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Anomaly Scores and Prediction Over Time',\n",
    "    xaxis_title='Timestamp',\n",
    "    yaxis_title='Anomaly Score',\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tabular View \n",
    "The same values can be seen in the following table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table\n",
    "fig_table = go.Figure(data=[go.Table(\n",
    "        header=dict(values=[\"Timestamp\", \"Score\", \"is Anomaly\"]),\n",
    "        cells=dict(values=[df['Timestamp'].astype(str), df['Score'].round(2), df['Is_Anomaly'].round(2)])\n",
    "    )])\n",
    "\n",
    "fig_table.update_layout(\n",
    "        title='Anomaly Scores Table'\n",
    "    ) \n",
    "    \n",
    "fig_table.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly scores and Prediction for each feature \n",
    "The following chart shows the individual anomaly scores for each feature which are plotted against the timestamp they were predicted at. The red area signifies the points which were predicted as anomalies overall. And the `Prediction` value shows if the individual point was flagged as an anomaly or not using that feature. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = predict_output\n",
    "\n",
    "# Extracting timestamps and prediction values\n",
    "timestamps = [datetime.fromisoformat(result['timestamp'].replace('Z', '+00:00')) for result in results]\n",
    "prediction_values_list = [result['prediction_values'] for result in results]\n",
    "\n",
    "# Find all unique feature names\n",
    "feature_names = set()\n",
    "for prediction_values in prediction_values_list:\n",
    "    for key in prediction_values.keys():\n",
    "        match = re.match(r'(Forecast|Recon|True)_(.+)', key)\n",
    "        if match:\n",
    "            feature_names.add(match.group(2))\n",
    "\n",
    "\n",
    "# Add traces for each feature\n",
    "for feature in feature_names:  \n",
    "    fig = go.Figure()        \n",
    "    pred_data = [] \n",
    "    score_data = []\n",
    "        \n",
    "    # Collect data for Threshold and Anomaly scores\n",
    "    for prediction_values in prediction_values_list:\n",
    "        pred_data.append(prediction_values.get(f'A_Pred_{feature}'))\n",
    "        score_data.append(prediction_values.get(f'A_Score_{feature}'))\n",
    "            \n",
    "    # Add Anomaly scores line\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=timestamps,\n",
    "            y=score_data, \n",
    "            mode='lines+markers',\n",
    "            hoverinfo='text',  # Show only text on hover\n",
    "            line=dict(color='#d53e1f', width=1),  # Line color\n",
    "            marker=dict(\n",
    "                size=4,  # Marker size\n",
    "                color='rgba(0,0,0,0)',  # Marker fill color (transparent)\n",
    "                line=dict(\n",
    "                    width=1,  # Border width\n",
    "                    color='#d53e1f'  # Border color (red)\n",
    "                ),\n",
    "            ), \n",
    "            name='Anomaly Score'\n",
    "        ))\n",
    " \n",
    " \n",
    "        \n",
    "    # Add Threshold line\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=timestamps,\n",
    "            y=pred_data,\n",
    "            name=\"Prediction\",\n",
    "            mode='lines+markers',\n",
    "            line=dict(color='green', width=1),  # Black line\n",
    "            marker=dict(\n",
    "                size=4,  # Marker size\n",
    "                color='rgba(0,0,0,0)',  # Marker fill color (transparent)\n",
    "                line=dict(\n",
    "                    width=1,  # Border width\n",
    "                    color='green'  # Border color (red)\n",
    "                ),\n",
    "            ), \n",
    "            #showlegend=False  # Do not show legend for the anomaly line\n",
    "        )) \n",
    "    \n",
    "  \n",
    "    # Add shaded areas for anomalies\n",
    "    for i in range(len(is_anomaly)):\n",
    "        if is_anomaly[i] == 1:\n",
    "            start = timestamps[i]\n",
    "            end = timestamps[i + 1] if i + 1 < len(timestamps) else timestamps[i]\n",
    "            fig.add_shape(\n",
    "                type=\"rect\",\n",
    "                x0=start,\n",
    "                x1=end,\n",
    "                y0=0,\n",
    "                y1=1,\n",
    "                xref='x',\n",
    "                yref='paper',\n",
    "                fillcolor='red',\n",
    "                opacity=0.2,\n",
    "                line_width=0,\n",
    "            )\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title_text=f\"Anomaly scores and Prediction for {feature}\",\n",
    "        height=400,  # Adjust height based on the number of features\n",
    "        width=1100,\n",
    "        showlegend=True\n",
    "    )\n",
    "\n",
    "    # Show plot\n",
    "    fig.show() \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecast and reconstruction vs true values for each feature \n",
    "The following chart shows the forecast and reconstruction vs true values which are plotted against the timestamp they were predicted at for each feature.  The red area signifies the points which were predicted as anomalies overall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = predict_output\n",
    "\n",
    "# Extracting timestamps and prediction values\n",
    "timestamps = [datetime.fromisoformat(result['timestamp'].replace('Z', '+00:00')) for result in results]\n",
    "prediction_values_list = [result['prediction_values'] for result in results]\n",
    "is_anomaly = [1 if result['is_anomaly'] else 0 for result in results]\n",
    "\n",
    "\n",
    "# Find all unique feature names\n",
    "feature_names = set()\n",
    "for prediction_values in prediction_values_list:\n",
    "    for key in prediction_values.keys():\n",
    "        match = re.match(r'(Forecast|Recon|True)_(.+)', key)\n",
    "        if match:\n",
    "            feature_names.add(match.group(2))\n",
    "\n",
    "\n",
    "# Plot each feature\n",
    "for feature in feature_names:\n",
    "    fig = go.Figure()        \n",
    "    forecast_data = []\n",
    "    recon_data = [] \n",
    "    true_data = []\n",
    "        \n",
    "    \n",
    "    for prediction_values in prediction_values_list:\n",
    "        forecast_data.append(prediction_values.get(f'Forecast_{feature}'))\n",
    "        recon_data.append(prediction_values.get(f'Recon_{feature}'))\n",
    "        true_data.append(prediction_values.get(f'True_{feature}'))\n",
    "\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=timestamps,\n",
    "            y=true_data,\n",
    "            line_color=\"rgb(0, 204, 150, 0.5)\",\n",
    "            name=\"True\",\n",
    "            line=dict(width=2),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=timestamps,\n",
    "            y=forecast_data,\n",
    "            line_color=\"rgb(255, 127, 14, 1)\",\n",
    "            name=\"Forecast\",\n",
    "            line=dict(width=2),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=timestamps,\n",
    "            y=recon_data,\n",
    "            line_color=\"rgb(31, 119, 180, 1)\",\n",
    "            name=\"Recon\",\n",
    "            line=dict(width=2),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Add shaded areas for anomalies\n",
    "    for i in range(len(is_anomaly)):\n",
    "        if is_anomaly[i] == 1:\n",
    "            start = timestamps[i]\n",
    "            end = timestamps[i + 1] if i + 1 < len(timestamps) else timestamps[i]\n",
    "            fig.add_shape(\n",
    "                type=\"rect\",\n",
    "                x0=start,\n",
    "                x1=end,\n",
    "                y0=0,\n",
    "                y1=1,\n",
    "                xref='x',\n",
    "                yref='paper',\n",
    "                fillcolor='red',\n",
    "                opacity=0.2,\n",
    "                line_width=0,\n",
    "            )\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title_text=f\"Forecast & reconstruction vs true values for {feature}\",\n",
    "        height=400,  # Adjust height based on the number of features\n",
    "        width=1100,\n",
    "        showlegend=True\n",
    "    )\n",
    "\n",
    "    # Show plot\n",
    "    fig.show() \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly scores and Thresholds over time \n",
    "The following chart shows the global anomaly scores for each data point which are plotted against the timestamp they were predicted at. The red area signifies the points which were predicted as anomalies overall. And the `Threshold` value shows the global threshold value for that point which the anomaly score was compared against. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each entry in the list\n",
    "results = predict_output\n",
    "    \n",
    "\n",
    "# Extracting timestamps and scores\n",
    "timestamps = [datetime.fromisoformat(result['timestamp'].replace('Z', '+00:00')) for result in results]\n",
    "scores = [result['score'] for result in results] \n",
    "thresholds = [result['prediction_values']['Thresh_Global'] for result in results] \n",
    "# Convert is_anomaly to 0 or 1\n",
    "is_anomaly = [1 if result['is_anomaly'] else 0 for result in results]\n",
    "\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({'Timestamp': timestamps, 'Score': scores, 'Is_Anomaly': is_anomaly, \"Threshold\": thresholds})\n",
    "\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add the score line with markers\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df['Timestamp'],\n",
    "    y=df['Score'],\n",
    "    text=df['Score'].apply(lambda x: f'Anomaly Score: {x:.2f}'),  # Hover text\n",
    "    mode='lines+markers',\n",
    "            hoverinfo='text',  # Show only text on hover\n",
    "            line=dict(color='#d53e1f', width=1),  # Line color\n",
    "            marker=dict(\n",
    "                size=4,  # Marker size\n",
    "                color='rgba(0,0,0,0)',  # Marker fill color (transparent)\n",
    "                line=dict(\n",
    "                    width=1,  # Border width\n",
    "                    color='#d53e1f'  # Border color (red)\n",
    "                ),\n",
    "            ), \n",
    "            name='Anomaly Score'\n",
    "        ))\n",
    "\n",
    "\n",
    "\n",
    "# Add the is_anomaly line\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df['Timestamp'],\n",
    "    y=df['Threshold'],\n",
    "    name=\"Threshold\",\n",
    "    line=dict(color=\"black\", width=1, dash=\"dash\"), \n",
    "    #showlegend=False  # Do not show legend for the anomaly line\n",
    "))\n",
    "\n",
    "# Add shaded areas for anomalies\n",
    "for i in range(len(df)):\n",
    "    if df['Is_Anomaly'][i] == 1:\n",
    "        start = df['Timestamp'][i]\n",
    "        end = df['Timestamp'][i + 1] if i + 1 < len(df) else df['Timestamp'][i]\n",
    "        fig.add_shape(\n",
    "            type=\"rect\",\n",
    "            x0=start,\n",
    "            x1=end,\n",
    "            y0=0,\n",
    "            y1=1,\n",
    "            xref='x',\n",
    "            yref='paper',\n",
    "            fillcolor='red',\n",
    "            opacity=0.2,\n",
    "            line_width=0,\n",
    "        )\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Anomaly Scores and Thresholds Over Time',\n",
    "    xaxis_title='Timestamp',\n",
    "    yaxis_title='Anomaly Score',\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly scores and Thresholds for each features\n",
    "The following chart shows the individual anomaly scores for each feature which are plotted against the timestamp they were predicted at. The red area signifies the points which were predicted as anomalies overall. And the `Threshold` value shows the threshold value for individual feature for that point which the anomaly score was compared against.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = predict_output\n",
    "\n",
    "# Extracting timestamps and prediction values\n",
    "timestamps = [datetime.fromisoformat(result['timestamp'].replace('Z', '+00:00')) for result in results]\n",
    "prediction_values_list = [result['prediction_values'] for result in results]\n",
    "\n",
    "# Find all unique feature names\n",
    "feature_names = set()\n",
    "for prediction_values in prediction_values_list:\n",
    "    for key in prediction_values.keys():\n",
    "        match = re.match(r'(Forecast|Recon|True)_(.+)', key)\n",
    "        if match:\n",
    "            feature_names.add(match.group(2))\n",
    "\n",
    "\n",
    "# Add traces for each feature\n",
    "for feature in feature_names:  \n",
    "    fig = go.Figure()        \n",
    "    thresh_data = [] \n",
    "    score_data = []\n",
    "        \n",
    "    # Collect data for Threshold and Anomaly scores\n",
    "    for prediction_values in prediction_values_list:\n",
    "        thresh_data.append(prediction_values.get(f'Thresh_{feature}'))\n",
    "        score_data.append(prediction_values.get(f'A_Score_{feature}'))\n",
    "            \n",
    "    # Add Anomaly scores line\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=timestamps,\n",
    "            y=score_data,\n",
    "            mode='lines+markers',\n",
    "            hoverinfo='text',  # Show only text on hover\n",
    "            line=dict(color='#d53e1f', width=1),  # Line color\n",
    "            marker=dict(\n",
    "                size=4,  # Marker size\n",
    "                color='rgba(0,0,0,0)',  # Marker fill color (transparent)\n",
    "                line=dict(\n",
    "                    width=1,  # Border width\n",
    "                    color='#d53e1f'  # Border color (red)\n",
    "                ),\n",
    "            ), \n",
    "            name='Anomaly Score'\n",
    "        ))\n",
    "    \n",
    "\n",
    "    # Add Threshold line\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=timestamps,\n",
    "            y=thresh_data,\n",
    "            name=\"Threshold\",\n",
    "            line=dict(color=\"black\", width=1, dash=\"dash\"), \n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Add shaded areas for anomalies\n",
    "    for i in range(len(is_anomaly)):\n",
    "        if is_anomaly[i] == 1:\n",
    "            start = timestamps[i]\n",
    "            end = timestamps[i + 1] if i + 1 < len(timestamps) else timestamps[i]\n",
    "            fig.add_shape(\n",
    "                type=\"rect\",\n",
    "                x0=start,\n",
    "                x1=end,\n",
    "                y0=0,\n",
    "                y1=1,\n",
    "                xref='x',\n",
    "                yref='paper',\n",
    "                fillcolor='red',\n",
    "                opacity=0.2,\n",
    "                line_width=0,\n",
    "            )\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title_text=f\"Anomaly scores and Thresholds for {feature}\",\n",
    "        height=400,  # Adjust height based on the number of features\n",
    "        width=1100,\n",
    "        showlegend=True\n",
    "    )\n",
    "\n",
    "    # Show plot\n",
    "    fig.show() \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook provides a comprehensive overview of the outputs and artifacts generated by ADBox when executed with a specific use case file. It includes detailed explanations and result plots that illustrate various methods for analyzing the algorithm's outputs. These plots help in understanding how the different factors and values used by ADBox to flag anomalies are interrelated on both a global and individual level.  This consolidated analysis helps users interpret the results and understand the effectiveness of the anomaly detection process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "siem-mtad-gat-2crsfNaz-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
